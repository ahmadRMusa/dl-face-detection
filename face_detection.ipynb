{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Segmentation (Face Detection) using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset_path = 'face_detection_dataset/'\n",
    "positive_eg = 'positive_bw/'\n",
    "negative_eg = 'negative_bw/'\n",
    "\n",
    "\n",
    "def encode_label(path):\n",
    "    # path_segments = path.split('/')\n",
    "    if 'positive' in path:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    return [int(label)]\n",
    "\n",
    "\n",
    "def read_label_dir(path):\n",
    "    # print(path)\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    # print(path + '*.png')\n",
    "    # print(glob(path + '*.png'))\n",
    "    for filepath in glob(path + '*.png'):\n",
    "        filepaths.append(filepath)\n",
    "        labels.append(encode_label(filepath))\n",
    "    return filepaths, labels\n",
    "\n",
    "# print(dataset_path + positive_eg)\n",
    "# print(read_label_dir(dataset_path + positive_eg))\n",
    "# print(read_label_dir(dataset_path + negative_eg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Start Building Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Examples: 24757\n",
      "Negative Examples: 121648\n"
     ]
    }
   ],
   "source": [
    "pos_filepaths, pos_labels =\\\n",
    "    read_label_dir(dataset_path + positive_eg)\n",
    "print('Positive Examples: %d' % len(pos_labels))\n",
    "neg_filepaths, neg_labels =\\\n",
    "    read_label_dir(dataset_path + negative_eg)\n",
    "print('Negative Examples: %d' % len(neg_labels))\n",
    "\n",
    "# all_filepaths = pos_filepaths + neg_filepaths\n",
    "# all_labels = pos_labels + neg_labels\n",
    "\n",
    "# Convert string into tensors\n",
    "pos_images = ops.convert_to_tensor(pos_filepaths, dtype=dtypes.string)\n",
    "pos_labels = ops.convert_to_tensor(pos_labels, dtype=dtypes.int32)\n",
    "\n",
    "neg_images = ops.convert_to_tensor(neg_filepaths, dtype=dtypes.string)\n",
    "neg_labels = ops.convert_to_tensor(neg_labels, dtype=dtypes.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Partitioning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_set_size = 1200\n",
    "neg_test_size = ceil(test_set_size / 6)\n",
    "pos_test_size = test_set_size - neg_test_size\n",
    "\n",
    "# Positive Examples\n",
    "# Create a partition vector\n",
    "pos_partitions = [0] * len(pos_filepaths)\n",
    "# print(partitions)\n",
    "pos_partitions[:pos_test_size] = [1] * pos_test_size\n",
    "# print(partitions)\n",
    "random.shuffle(pos_partitions)\n",
    "# print(partitions)\n",
    "\n",
    "# Partition data into a test and train set according to partition vector\n",
    "pos_train_images, pos_test_images = tf.dynamic_partition(pos_images, pos_partitions, 2)\n",
    "pos_train_labels, pos_test_labels = tf.dynamic_partition(pos_labels, pos_partitions, 2)\n",
    "\n",
    "# Negative Examples\n",
    "# Create a partition vector\n",
    "neg_partitions = [0] * len(neg_filepaths)\n",
    "# print(partitions)\n",
    "neg_partitions[:neg_test_size] = [1] * neg_test_size\n",
    "# print(partitions)\n",
    "random.shuffle(neg_partitions)\n",
    "# print(partitions)\n",
    "\n",
    "# Partition data into a test and train set according to partition vector\n",
    "neg_train_images, neg_test_images = tf.dynamic_partition(neg_images, neg_partitions, 2)\n",
    "neg_train_labels, neg_test_labels = tf.dynamic_partition(neg_labels, neg_partitions, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Input Queues and Define How to Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 1\n",
    "\n",
    "# Create input queues\n",
    "pos_train_queue = tf.train.slice_input_producer(\n",
    "                                    [pos_train_images, pos_train_labels],\n",
    "                                    shuffle=False)\n",
    "pos_test_queue = tf.train.slice_input_producer(\n",
    "                                    [pos_test_images, pos_test_labels],\n",
    "                                    shuffle=False)\n",
    "\n",
    "# Process path and string tensor into an image and a label\n",
    "pos_file_content = tf.read_file(pos_train_queue[0])\n",
    "pos_train_image = tf.image.decode_png(pos_file_content, channels=NUM_CHANNELS)\n",
    "pos_train_label = pos_train_queue[1]\n",
    "\n",
    "pos_file_content = tf.read_file(pos_test_queue[0])\n",
    "pos_test_image = tf.image.decode_png(pos_file_content, channels=NUM_CHANNELS)\n",
    "pos_test_label = pos_test_queue[1]\n",
    "\n",
    "# Create negative input queues\n",
    "neg_train_queue = tf.train.slice_input_producer(\n",
    "                                    [neg_train_images, neg_train_labels],\n",
    "                                    shuffle=False)\n",
    "neg_test_queue = tf.train.slice_input_producer(\n",
    "                                    [neg_test_images, neg_test_labels],\n",
    "                                    shuffle=False)\n",
    "\n",
    "# Process path and string tensor into an image and a label\n",
    "neg_file_content = tf.read_file(neg_train_queue[0])\n",
    "neg_train_image = tf.image.decode_png(neg_file_content, channels=NUM_CHANNELS)\n",
    "neg_train_label = neg_train_queue[1]\n",
    "\n",
    "neg_file_content = tf.read_file(neg_test_queue[0])\n",
    "neg_test_image = tf.image.decode_png(neg_file_content, channels=NUM_CHANNELS)\n",
    "neg_test_label = neg_test_queue[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Group Samples into Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_WIDTH = 128\n",
    "BATCH_SIZE = 240\n",
    "NEG_BATCH_SIZE = ceil(BATCH_SIZE / 6)\n",
    "POS_BATCH_SIZE = BATCH_SIZE - NEG_BATCH_SIZE\n",
    "\n",
    "# Define tensor shape\n",
    "pos_train_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])\n",
    "pos_test_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])\n",
    "\n",
    "neg_train_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])\n",
    "neg_test_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])\n",
    "\n",
    "# Collect batches of images before processing\n",
    "pos_train_image_batch, pos_train_label_batch = tf.train.batch(\n",
    "                                    [pos_train_image, pos_train_label],\n",
    "                                    batch_size=POS_BATCH_SIZE\n",
    "                                    # ,num_threads=1\n",
    "                                    )\n",
    "pos_test_image_batch, pos_test_label_batch = tf.train.batch(\n",
    "                                    [pos_test_image, pos_test_label],\n",
    "                                    batch_size=POS_BATCH_SIZE\n",
    "                                    # ,num_threads=1\n",
    "                                    )\n",
    "\n",
    "neg_train_image_batch, neg_train_label_batch = tf.train.batch(\n",
    "                                    [neg_train_image, neg_train_label],\n",
    "                                    batch_size=NEG_BATCH_SIZE\n",
    "                                    # ,num_threads=1\n",
    "                                    )\n",
    "neg_test_image_batch, neg_test_label_batch = tf.train.batch(\n",
    "                                    [neg_test_image, neg_test_label],\n",
    "                                    batch_size=NEG_BATCH_SIZE\n",
    "                                    # ,num_threads=1\n",
    "                                    )\n",
    "\n",
    "# Join the postive and negative batches\n",
    "train_image_batch = tf.concat([pos_train_image_batch, neg_train_image_batch], 0)\n",
    "train_label_batch = tf.concat([pos_train_label_batch, neg_train_label_batch], 0)\n",
    "test_image_batch = tf.concat([pos_test_image_batch, neg_test_image_batch], 0)\n",
    "test_label_batch = tf.concat([pos_test_label_batch, neg_test_label_batch], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Run the Queue Runners and Start a Session\n",
    "- **Note:** This section is meant for testing only. Do not run during main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the train set:\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "From the test set:\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # print(sess.run(all_images))\n",
    "\n",
    "    # Initialize the queue threads to start to shovel data\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    print(\"From the train set:\")\n",
    "    for i in range(1):\n",
    "        print(sess.run(train_label_batch))\n",
    "\n",
    "    print(\"From the test set:\")\n",
    "    for i in range(1):\n",
    "        print(sess.run(test_label_batch))\n",
    "\n",
    "    # Stop our queue threads and properly close the session\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
