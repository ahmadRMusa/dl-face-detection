{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Segmentation (Face Detection) using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from time import time\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset_path = 'face_detection_dataset/'\n",
    "positive_eg = 'positive_bw/'\n",
    "negative_eg = 'negative_bw/'\n",
    "\n",
    "\n",
    "def encode_label(path):\n",
    "    # path_segments = path.split('/')\n",
    "    if 'positive' in path:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    return [int(label)]\n",
    "\n",
    "\n",
    "def read_label_dir(path):\n",
    "    # print(path)\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    # print(path + '*.png')\n",
    "    # print(glob(path + '*.png'))\n",
    "    for filepath in glob(path + '*.png'):\n",
    "        filepaths.append(filepath)\n",
    "        labels.append(encode_label(filepath))\n",
    "    return filepaths, labels\n",
    "\n",
    "# print(dataset_path + positive_eg)\n",
    "# print(read_label_dir(dataset_path + positive_eg))\n",
    "# print(read_label_dir(dataset_path + negative_eg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Start Building Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Examples: 24757\n",
      "Negative Examples: 121648\n"
     ]
    }
   ],
   "source": [
    "pos_filepaths, pos_labels =\\\n",
    "    read_label_dir(dataset_path + positive_eg)\n",
    "print('Positive Examples: %d' % len(pos_labels))\n",
    "neg_filepaths, neg_labels =\\\n",
    "    read_label_dir(dataset_path + negative_eg)\n",
    "print('Negative Examples: %d' % len(neg_labels))\n",
    "\n",
    "# all_filepaths = pos_filepaths + neg_filepaths\n",
    "# all_labels = pos_labels + neg_labels\n",
    "\n",
    "# Convert string into tensors\n",
    "pos_images = ops.convert_to_tensor(pos_filepaths, dtype=dtypes.string)\n",
    "pos_labels = ops.convert_to_tensor(pos_labels, dtype=dtypes.int32)\n",
    "\n",
    "neg_images = ops.convert_to_tensor(neg_filepaths, dtype=dtypes.string)\n",
    "neg_labels = ops.convert_to_tensor(neg_labels, dtype=dtypes.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Partitioning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_set_size = 1200\n",
    "pos_test_size = ceil(test_set_size / 6)\n",
    "neg_test_size = test_set_size - pos_test_size\n",
    "\n",
    "# Positive Examples\n",
    "# Create a partition vector\n",
    "pos_partitions = [0] * len(pos_filepaths)\n",
    "# print(partitions)\n",
    "pos_partitions[:pos_test_size] = [1] * pos_test_size\n",
    "# print(partitions)\n",
    "random.shuffle(pos_partitions)\n",
    "# print(partitions)\n",
    "\n",
    "# Partition data into a test and train set according to partition vector\n",
    "pos_train_images, pos_test_images = tf.dynamic_partition(pos_images, pos_partitions, 2)\n",
    "pos_train_labels, pos_test_labels = tf.dynamic_partition(pos_labels, pos_partitions, 2)\n",
    "\n",
    "# Negative Examples\n",
    "# Create a partition vector\n",
    "neg_partitions = [0] * len(neg_filepaths)\n",
    "# print(partitions)\n",
    "neg_partitions[:neg_test_size] = [1] * neg_test_size\n",
    "# print(partitions)\n",
    "random.shuffle(neg_partitions)\n",
    "# print(partitions)\n",
    "\n",
    "# Partition data into a test and train set according to partition vector\n",
    "neg_train_images, neg_test_images = tf.dynamic_partition(neg_images, neg_partitions, 2)\n",
    "neg_train_labels, neg_test_labels = tf.dynamic_partition(neg_labels, neg_partitions, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Input Queues and Define How to Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 1\n",
    "\n",
    "# Create input queues\n",
    "pos_train_queue = tf.train.slice_input_producer(\n",
    "                                    [pos_train_images, pos_train_labels],\n",
    "                                    shuffle=False)\n",
    "pos_test_queue = tf.train.slice_input_producer(\n",
    "                                    [pos_test_images, pos_test_labels],\n",
    "                                    shuffle=False)\n",
    "\n",
    "# Process path and string tensor into an image and a label\n",
    "pos_file_content = tf.read_file(pos_train_queue[0])\n",
    "pos_train_image = tf.image.decode_png(pos_file_content, channels=NUM_CHANNELS)\n",
    "pos_train_label = pos_train_queue[1]\n",
    "\n",
    "pos_file_content = tf.read_file(pos_test_queue[0])\n",
    "pos_test_image = tf.image.decode_png(pos_file_content, channels=NUM_CHANNELS)\n",
    "pos_test_label = pos_test_queue[1]\n",
    "\n",
    "# Create negative input queues\n",
    "neg_train_queue = tf.train.slice_input_producer(\n",
    "                                    [neg_train_images, neg_train_labels],\n",
    "                                    shuffle=False)\n",
    "neg_test_queue = tf.train.slice_input_producer(\n",
    "                                    [neg_test_images, neg_test_labels],\n",
    "                                    shuffle=False)\n",
    "\n",
    "# Process path and string tensor into an image and a label\n",
    "neg_file_content = tf.read_file(neg_train_queue[0])\n",
    "neg_train_image = tf.image.decode_png(neg_file_content, channels=NUM_CHANNELS)\n",
    "neg_train_label = neg_train_queue[1]\n",
    "\n",
    "neg_file_content = tf.read_file(neg_test_queue[0])\n",
    "neg_test_image = tf.image.decode_png(neg_file_content, channels=NUM_CHANNELS)\n",
    "neg_test_label = neg_test_queue[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Group Samples into Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_WIDTH = 128\n",
    "BATCH_SIZE = 240\n",
    "POS_BATCH_SIZE = ceil(BATCH_SIZE / 6)\n",
    "NEG_BATCH_SIZE = BATCH_SIZE - POS_BATCH_SIZE\n",
    "\n",
    "# Define tensor shape\n",
    "pos_train_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])\n",
    "pos_test_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])\n",
    "\n",
    "neg_train_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])\n",
    "neg_test_image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])\n",
    "\n",
    "# Collect batches of images before processing\n",
    "pos_train_image_batch, pos_train_label_batch = tf.train.batch(\n",
    "                                    [pos_train_image, pos_train_label],\n",
    "                                    batch_size=POS_BATCH_SIZE\n",
    "                                    # ,num_threads=1\n",
    "                                    )\n",
    "pos_test_image_batch, pos_test_label_batch = tf.train.batch(\n",
    "                                    [pos_test_image, pos_test_label],\n",
    "                                    batch_size=POS_BATCH_SIZE\n",
    "                                    # ,num_threads=1\n",
    "                                    )\n",
    "\n",
    "neg_train_image_batch, neg_train_label_batch = tf.train.batch(\n",
    "                                    [neg_train_image, neg_train_label],\n",
    "                                    batch_size=NEG_BATCH_SIZE\n",
    "                                    # ,num_threads=1\n",
    "                                    )\n",
    "neg_test_image_batch, neg_test_label_batch = tf.train.batch(\n",
    "                                    [neg_test_image, neg_test_label],\n",
    "                                    batch_size=NEG_BATCH_SIZE\n",
    "                                    # ,num_threads=1\n",
    "                                    )\n",
    "\n",
    "# Join the postive and negative batches\n",
    "train_image_batch = tf.concat([pos_train_image_batch, neg_train_image_batch], 0)\n",
    "train_label_batch = tf.concat([pos_train_label_batch, neg_train_label_batch], 0)\n",
    "test_image_batch = tf.concat([pos_test_image_batch, neg_test_image_batch], 0)\n",
    "test_label_batch = tf.concat([pos_test_label_batch, neg_test_label_batch], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Run the Queue Runners and Start a Session\n",
    "- **Note:** This section is meant for testing only. Do not run during main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # print(sess.run(all_images))\n",
    "\n",
    "    # Initialize the queue threads to start to shovel data\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    print(\"From the train set:\")\n",
    "    for i in range(1):\n",
    "        print(sess.run(train_label_batch))\n",
    "\n",
    "    print(\"From the test set:\")\n",
    "    for i in range(1):\n",
    "        print(sess.run(test_label_batch))\n",
    "\n",
    "    # Stop our queue threads and properly close the session\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define Placeholders and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 128, 128, 1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 5])\n",
    "b_conv1 = bias_variable([1])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 128, 128, 1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 5, 10])\n",
    "b_conv2 = bias_variable([10])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([32 * 32 * 10, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 32*32*10])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 1])\n",
    "b_fc2 = bias_variable([1])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "y_thres = tf.cast(y_conv + 0.5, tf.int32)\n",
    "y_ = tf.cast(y_, tf.int32)\n",
    "correct_prediction = tf.equal(y_thres, y_)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,  tf.float32))\n",
    "\n",
    "train_iterations = 10000\n",
    "test_iterations = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Initialize the queue threads to start to shovel data\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    '''\n",
    "    print('W_conv1: ' + str(sess.run(tf.shape(W_conv1))))\n",
    "    print('b_conv1: ' + str(sess.run(tf.shape(b_conv1))))\n",
    "    print('x_image: ' + str(sess.run(tf.shape(x_image), feed_dict = {x: train_image_batch.eval()})))\n",
    "    print('h_conv1: ' + str(sess.run(tf.shape(h_conv1), feed_dict = {x: train_image_batch.eval(), y_: train_label_batch.eval()})))\n",
    "    print('h_pool1: ' + str(sess.run(tf.shape(h_pool1), feed_dict = {x: train_image_batch.eval(), y_: train_label_batch.eval()})))\n",
    "    print('W_conv2: ' + str(sess.run(tf.shape(W_conv2))))\n",
    "    print('b_conv2: ' + str(sess.run(tf.shape(b_conv2))))\n",
    "    print('h_conv2: ' + str(sess.run(tf.shape(h_conv2), feed_dict = {x: train_image_batch.eval(), y_: train_label_batch.eval()})))\n",
    "    print('h_pool2: ' + str(sess.run(tf.shape(h_pool2), feed_dict = {x: train_image_batch.eval(), y_: train_label_batch.eval()})))\n",
    "    print('W_fc1: ' + str(sess.run(tf.shape(W_fc1))))\n",
    "    print('b_fc1: ' + str(sess.run(tf.shape(b_fc1))))\n",
    "    print('h_pool2_flat: ' + str(sess.run(tf.shape(h_pool2_flat), feed_dict = {x: train_image_batch.eval(), y_: train_label_batch.eval()})))\n",
    "    print('h_fc1: ' + str(sess.run(tf.shape(h_fc1), feed_dict = {x: train_image_batch.eval(), y_: train_label_batch.eval()})))\n",
    "    print('keep_prob: ' + str(sess.run(tf.shape(keep_prob), feed_dict = {x: train_image_batch.eval(), y_: train_label_batch.eval(), keep})))\n",
    "    print('h_fc1_drop: ' + str(sess.run(tf.shape(h_fc1_drop), feed_dict = {x: train_image_batch.eval(), y_: train_label_batch.eval(), keep_prob: 1.0})))\n",
    "    print('W_fc2: ' + str(sess.run(tf.shape(W_fc2))))\n",
    "    print('b_fc2: ' + str(sess.run(tf.shape(b_fc2))))\n",
    "    print('y_conv: ' + str(sess.run(tf.shape(y_conv), feed_dict = {x: train_image_batch.eval(), y_: train_label_batch.eval(), keep_prob: 1.0})))\n",
    "    '''\n",
    "    print(\"Training\")\n",
    "    for i in range(train_iterations):\n",
    "        start_time = time()\n",
    "        feed_dict = {x: train_image_batch.eval(),\n",
    "                     y_: train_label_batch.eval(),\n",
    "                     keep_prob: 1.0}\n",
    "        if i % 10 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict)\n",
    "            print(\"Step %d, Training accuracy %g\" % (i, train_accuracy))\n",
    "        feed_dict = {x: train_image_batch.eval(),\n",
    "                     y_: train_label_batch.eval(),\n",
    "                     keep_prob: 0.5}\n",
    "        train_step.run(feed_dict)\n",
    "        end_time = time()\n",
    "        print(\"Step %d, Training time %f\" % (i, end_time - start_time))\n",
    "\n",
    "    for i in range(test_iterations):\n",
    "        print(\"test accuracy %g\" % accuracy.eval(feed_dict={\n",
    "              x: test_image_batch.eval(), y_: test_label_batch.eval(),\n",
    "              keep_prob: 1.0}))\n",
    "\n",
    "    # Stop our queue threads and properly close the session\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
